#!/usr/bin/env python3
"""
è®ºæ–‡ä¸‹è½½å·¥å…· - ä»arXivé“¾æ¥æ‰¹é‡ä¸‹è½½PDFæ–‡ä»¶åˆ°papersç›®å½•
"""

import requests
import os
import re
from pathlib import Path
from typing import List, Dict
import logging

class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å™¨ - ä¸“é—¨ç”¨äºä¸‹è½½arXivè®ºæ–‡åˆ°papersç›®å½•"""
    
    def __init__(self, papers_dir: str = "papers"):
        self.papers_dir = papers_dir
        self.logger = logging.getLogger(__name__)
        
        # ç¡®ä¿papersç›®å½•å­˜åœ¨
        os.makedirs(papers_dir, exist_ok=True)
        
    def extract_arxiv_id_from_url(self, arxiv_url: str) -> str:
        """ä»arXiv URLä¸­æå–å®Œæ•´çš„arXiv IDï¼ˆåŒ…å«ç‰ˆæœ¬å·ï¼‰"""
        # åŒ¹é…arXiv URLæ ¼å¼: https://arxiv.org/abs/1706.03762 æˆ– https://arxiv.org/abs/1706.03762v2
        patterns = [
            r'https?://arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)',
            r'https?://arxiv\.org/pdf/([0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)',
            r'([0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, arxiv_url)
            if match:
                return match.group(1)
        
        raise ValueError(f"æ— æ³•ä»URLä¸­æå–arXiv ID: {arxiv_url}")
    
    def extract_base_arxiv_id(self, arxiv_id: str) -> str:
        """ä»å®Œæ•´arXiv IDä¸­æå–åŸºç¡€IDï¼ˆå»é™¤ç‰ˆæœ¬å·ï¼‰"""
        # å»é™¤ç‰ˆæœ¬å·ï¼Œä¾‹å¦‚ 1706.03762v2 -> 1706.03762
        return re.sub(r'v[0-9]+$', '', arxiv_id)
    
    def extract_version_number(self, arxiv_id: str) -> int:
        """ä»arXiv IDä¸­æå–ç‰ˆæœ¬å·ï¼Œå¦‚æœæ²¡æœ‰ç‰ˆæœ¬å·åˆ™è¿”å›1"""
        version_match = re.search(r'v([0-9]+)$', arxiv_id)
        return int(version_match.group(1)) if version_match else 1
    
    def find_existing_versions(self, base_arxiv_id: str) -> List[Dict[str, any]]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„è¯¥è®ºæ–‡çš„æ‰€æœ‰ç‰ˆæœ¬"""
        existing_versions = []
        
        # æ£€æŸ¥papersç›®å½•ä¸­çš„æ–‡ä»¶
        for filename in os.listdir(self.papers_dir):
            if filename.startswith(base_arxiv_id) and filename.endswith('.pdf'):
                file_path = os.path.join(self.papers_dir, filename)
                # æå–ç‰ˆæœ¬å·
                version_match = re.search(f'{re.escape(base_arxiv_id)}(?:v([0-9]+))?\\.pdf$', filename)
                version = int(version_match.group(1)) if version_match and version_match.group(1) else 1
                
                existing_versions.append({
                    'filename': filename,
                    'path': file_path,
                    'version': version,
                    'size': os.path.getsize(file_path)
                })
        
        return sorted(existing_versions, key=lambda x: x['version'], reverse=True)
    
    def build_pdf_url(self, arxiv_id: str) -> str:
        """æ ¹æ®arXiv IDæ„å»ºPDFä¸‹è½½é“¾æ¥"""
        return f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    
    def get_paper_filename(self, arxiv_id: str, use_base_id: bool = True) -> str:
        """æ ¹æ®arXiv IDç”Ÿæˆæ–‡ä»¶å"""
        if use_base_id:
            # ä½¿ç”¨åŸºç¡€IDï¼ˆå»é™¤ç‰ˆæœ¬å·ï¼‰ä½œä¸ºæ–‡ä»¶å
            base_id = self.extract_base_arxiv_id(arxiv_id)
            return f"{base_id}.pdf"
        else:
            # ä½¿ç”¨å®Œæ•´IDï¼ˆåŒ…å«ç‰ˆæœ¬å·ï¼‰ä½œä¸ºæ–‡ä»¶å
            return f"{arxiv_id}.pdf"
    
    def download_single_paper(self, arxiv_url: str, force_latest: bool = True) -> Dict[str, any]:
        """ä¸‹è½½å•ä¸ªè®ºæ–‡ï¼Œæ™ºèƒ½å¤„ç†ç‰ˆæœ¬"""
        try:
            # 1. æå–å®Œæ•´arXiv IDï¼ˆå¯èƒ½åŒ…å«ç‰ˆæœ¬å·ï¼‰
            full_arxiv_id = self.extract_arxiv_id_from_url(arxiv_url)
            base_arxiv_id = self.extract_base_arxiv_id(full_arxiv_id)
            current_version = self.extract_version_number(full_arxiv_id)
            
            # 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥è®ºæ–‡çš„å…¶ä»–ç‰ˆæœ¬
            existing_versions = self.find_existing_versions(base_arxiv_id)
            
            if existing_versions:
                latest_existing = existing_versions[0]  # å·²æŒ‰ç‰ˆæœ¬å·é™åºæ’åº
                existing_version = latest_existing['version']
                
                # å¦‚æœå·²æœ‰æ›´æ–°æˆ–ç›¸åŒç‰ˆæœ¬ï¼Œè·³è¿‡ä¸‹è½½
                if existing_version >= current_version:
                    return {
                        "status": "exists",
                        "arxiv_id": base_arxiv_id,
                        "full_arxiv_id": full_arxiv_id,
                        "filename": latest_existing['filename'],
                        "file_path": latest_existing['path'],
                        "size": latest_existing['size'],
                        "existing_version": existing_version,
                        "requested_version": current_version,
                        "message": f"å·²å­˜åœ¨æ›´æ–°ç‰ˆæœ¬ v{existing_version}ï¼ˆè¯·æ±‚ç‰ˆæœ¬ v{current_version}ï¼‰: {latest_existing['filename']}"
                    }
                
                # å¦‚æœè¯·æ±‚çš„æ˜¯æ›´æ–°ç‰ˆæœ¬ï¼Œåˆ é™¤æ—§ç‰ˆæœ¬
                if force_latest and current_version > existing_version:
                    for old_version in existing_versions:
                        old_file_path = old_version['path']
                        old_md_path = old_file_path + '.md'
                        
                        self.logger.info(f"åˆ é™¤æ—§ç‰ˆæœ¬ v{old_version['version']}: {old_version['filename']}")
                        os.remove(old_file_path)
                        
                        # åŒæ—¶åˆ é™¤å¯¹åº”çš„mdæ–‡ä»¶
                        if os.path.exists(old_md_path):
                            os.remove(old_md_path)
                            self.logger.info(f"åˆ é™¤æ—§ç‰ˆæœ¬è§£ææ–‡ä»¶: {old_version['filename']}.md")
            
            # 3. ç”Ÿæˆæ–°æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨åŸºç¡€IDï¼‰
            filename = self.get_paper_filename(full_arxiv_id, use_base_id=True)
            file_path = os.path.join(self.papers_dir, filename)
            
            # 4. æ„å»ºPDFä¸‹è½½é“¾æ¥
            pdf_url = self.build_pdf_url(full_arxiv_id)
            
            # 5. ä¸‹è½½æ–‡ä»¶
            self.logger.info(f"ä¸‹è½½è®ºæ–‡: {full_arxiv_id} -> {filename}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Linux; x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # 6. ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            
            version_info = f" (v{current_version})" if current_version > 1 else ""
            
            return {
                "status": "success",
                "arxiv_id": base_arxiv_id,
                "full_arxiv_id": full_arxiv_id,
                "filename": filename,
                "file_path": file_path,
                "size": file_size,
                "version": current_version,
                "message": f"ä¸‹è½½æˆåŠŸ{version_info}: {filename}"
            }
            
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            self.logger.error(f"ä¸‹è½½è®ºæ–‡å¤±è´¥ {arxiv_url}: {e}")
            
            return {
                "status": "error",
                "arxiv_url": arxiv_url,
                "message": error_msg,
                "error": str(e)
            }
    
    def download_papers_batch(self, arxiv_urls: List[str]) -> Dict[str, any]:
        """æ‰¹é‡ä¸‹è½½è®ºæ–‡"""
        results = {
            "total": len(arxiv_urls),
            "success": 0,
            "exists": 0,
            "failed": 0,
            "details": []
        }
        
        print(f"ğŸ“¥ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(arxiv_urls)} ç¯‡è®ºæ–‡åˆ° {self.papers_dir}/ ç›®å½•")
        print("=" * 60)
        
        for i, arxiv_url in enumerate(arxiv_urls, 1):
            print(f"\nğŸ“„ [{i}/{len(arxiv_urls)}] å¤„ç†: {arxiv_url}")
            
            result = self.download_single_paper(arxiv_url)
            results["details"].append(result)
            
            if result["status"] == "success":
                results["success"] += 1
                print(f"âœ… {result['message']} ({result['size']} bytes)")
            elif result["status"] == "exists":
                results["exists"] += 1
                print(f"â„¹ï¸  {result['message']} ({result['size']} bytes)")
            else:
                results["failed"] += 1
                print(f"âŒ {result['message']}")
        
        # æ˜¾ç¤ºæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸‹è½½ç»“æœæ€»ç»“:")
        print(f"  âœ… æ–°ä¸‹è½½: {results['success']} ç¯‡")
        print(f"  â„¹ï¸  å·²å­˜åœ¨: {results['exists']} ç¯‡")
        print(f"  âŒ å¤±è´¥: {results['failed']} ç¯‡")
        print(f"  ğŸ“ æ€»è®¡: {results['total']} ç¯‡")
        
        return results

def download_paper_list(paper_urls: List[str], papers_dir: str = "papers") -> Dict[str, any]:
    """
    ç®€å•çš„è®ºæ–‡æ‰¹é‡ä¸‹è½½å‡½æ•°
    
    Args:
        paper_urls (List[str]): arXivè®ºæ–‡URLåˆ—è¡¨
        papers_dir (str): ä¸‹è½½ç›®å½•ï¼Œé»˜è®¤ä¸ºpapers
        
    Returns:
        Dict[str, any]: ä¸‹è½½ç»“æœç»Ÿè®¡
    """
    downloader = PaperDownloader(papers_dir)
    return downloader.download_papers_batch(paper_urls)

# æ¼”ç¤ºç”¨æ³•
if __name__ == "__main__":
    # æµ‹è¯•è®ºæ–‡åˆ—è¡¨
    test_papers = [
        "https://arxiv.org/abs/1706.03762",  # Attention Is All You Need
        "https://arxiv.org/abs/1810.04805",  # BERT
        "https://arxiv.org/abs/2005.14165",  # GPT-3
    ]
    
    print("ğŸš€ è®ºæ–‡ä¸‹è½½å·¥å…·æ¼”ç¤º")
    result = download_paper_list(test_papers)
    
    print(f"\nğŸ¯ ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {result['success']}, å·²å­˜åœ¨: {result['exists']}, å¤±è´¥: {result['failed']}") 