#!/usr/bin/env python3
"""
ä»papersç›®å½•ä¸‹çš„mdæ–‡ä»¶ç›´æ¥æå–æ¦‚å¿µå¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
è·³è¿‡æ•°æ®åº“å­˜å‚¨ï¼Œç›´æ¥ç”Ÿæˆæœ¬åœ°JSONæ–‡ä»¶
"""

import os
import sys
import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥agentsæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from agents.concept_extraction_agent import ConceptExtractionAgent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def parse_markdown_paper(md_file_path: str) -> Optional[Dict]:
    """è§£æmarkdownæ–‡ä»¶ï¼Œæå–è®ºæ–‡ä¿¡æ¯"""
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–arXiv IDä»æ–‡ä»¶å
        arxiv_id = Path(md_file_path).stem.replace('.pdf', '')
        
        # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨æ–‡ä»¶å¼€å¤´å‡ è¡Œï¼Œä¸åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è¾ƒçŸ­è¡Œï¼‰
        title = extract_title_from_content(content)
        
        # æå–æ‘˜è¦
        abstract = extract_abstract_from_content(content)
        
        # æå–ä½œè€…ä¿¡æ¯
        authors = extract_authors_from_content(content)
        
        paper_info = {
            'arxiv_id': arxiv_id,
            'title': title,
            'authors': authors,
            'abstract': abstract,
            'parsed_text': content,
            'year': extract_year_from_content(content),
            'citation_count': 0,  # é»˜è®¤å€¼
            'journal': 'arXiv preprint',
            'doi': ''
        }
        
        return paper_info
        
    except Exception as e:
        logger.error(f"è§£æmarkdownæ–‡ä»¶å¤±è´¥ {md_file_path}: {e}")
        return None

def extract_title_from_content(content: str) -> str:
    """ä»å†…å®¹ä¸­æå–æ ‡é¢˜"""
    lines = content.split('\n')
    
    # è·³è¿‡æˆæƒä¿¡æ¯ç­‰ï¼Œå¯»æ‰¾åˆé€‚çš„æ ‡é¢˜
    for i, line in enumerate(lines[:20]):
        line = line.strip()
        
        # æ’é™¤ä¸€äº›æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„è¡Œ
        if (len(line) > 10 and len(line) < 200 and 
            not line.startswith('Provided') and
            not line.startswith('Google') and
            not line.startswith('Abstract') and
            not '@' in line and
            not line.startswith('*') and
            not line.startswith('â€ ') and
            not line.startswith('â€¡') and
            not re.match(r'^[A-Z][a-z]+ [A-Z]', line)):  # æ’é™¤ä½œè€…å
            
            # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒæ ‡é¢˜
            if (any(word in line.lower() for word in ['attention', 'neural', 'learning', 'network', 'model', 'transformer', 'bert', 'gpt']) or
                len(line.split()) > 2):
                return line
    
    return "æœªçŸ¥æ ‡é¢˜"

def extract_abstract_from_content(content: str) -> str:
    """ä»å†…å®¹ä¸­æå–æ‘˜è¦"""
    # å¯»æ‰¾Abstractæ ‡è®°
    abstract_match = re.search(r'Abstract\s*\n(.*?)(?=\n\n|\n[A-Z]|\n\d+\s)', content, re.DOTALL)
    
    if abstract_match:
        abstract = abstract_match.group(1).strip()
        # æ¸…ç†æ‘˜è¦ï¼Œç§»é™¤ä½œè€…ä¿¡æ¯ç­‰
        abstract = re.sub(r'\*.*?\n', '', abstract)  # ç§»é™¤æ˜Ÿå·æ ‡è®°çš„è¡Œ
        abstract = re.sub(r'â€ .*?\n', '', abstract)   # ç§»é™¤â€ æ ‡è®°çš„è¡Œ
        abstract = re.sub(r'â€¡.*?\n', '', abstract)   # ç§»é™¤â€¡æ ‡è®°çš„è¡Œ
        abstract = re.sub(r'\n+', ' ', abstract)     # åˆå¹¶å¤šè¡Œ
        
        if len(abstract) > 50:  # ç¡®ä¿æ‘˜è¦è¶³å¤Ÿé•¿
            return abstract[:1000]  # é™åˆ¶é•¿åº¦
    
    # å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†çš„Abstractï¼Œå°è¯•æå–ç¬¬ä¸€æ®µè¾ƒé•¿çš„æ–‡æœ¬
    paragraphs = content.split('\n\n')
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if len(paragraph) > 100 and 'Â©' not in paragraph and 'Google' not in paragraph:
            return paragraph[:1000]
    
    return "æœªæ‰¾åˆ°æ‘˜è¦"

def extract_authors_from_content(content: str) -> List[str]:
    """ä»å†…å®¹ä¸­æå–ä½œè€…ä¿¡æ¯"""
    authors = []
    lines = content.split('\n')
    
    # å¯»æ‰¾åŒ…å«@çš„è¡Œï¼ˆé‚®ç®±ï¼‰æˆ–çœ‹èµ·æ¥åƒä½œè€…åçš„è¡Œ
    for line in lines[:30]:
        line = line.strip()
        
        # åŒ¹é…ä½œè€…åæ¨¡å¼ï¼ˆåå­—+å§“æ°+å¯èƒ½çš„æœºæ„ï¼‰
        if re.match(r'^[A-Z][a-z]+\s+[A-Z]', line) and '@' not in line:
            # æ¸…ç†ä½œè€…å
            author = re.sub(r'\s*âˆ—.*', '', line)  # ç§»é™¤æ˜Ÿå·åŠåç»­å†…å®¹
            author = re.sub(r'\s*â€ .*', '', author)  # ç§»é™¤â€ åŠåç»­å†…å®¹
            author = re.sub(r'\s*â€¡.*', '', author)  # ç§»é™¤â€¡åŠåç»­å†…å®¹
            author = re.sub(r'\s+Google.*', '', author)  # ç§»é™¤æœºæ„ä¿¡æ¯
            author = re.sub(r'\s+University.*', '', author)  # ç§»é™¤å¤§å­¦ä¿¡æ¯
            
            if len(author.split()) >= 2:  # è‡³å°‘æœ‰åå’Œå§“
                authors.append(author.strip())
        
        # å¦‚æœæ‰¾åˆ°äº†ä¸€äº›ä½œè€…ï¼Œåœæ­¢æœç´¢
        if len(authors) >= 3:
            break
    
    return authors if authors else ["æœªçŸ¥ä½œè€…"]

def extract_year_from_content(content: str) -> Optional[int]:
    """ä»å†…å®¹ä¸­æå–å¹´ä»½"""
    # å¯»æ‰¾å¹´ä»½æ¨¡å¼
    year_patterns = [
        r'20[0-2][0-9]',  # 2000-2029
        r'19[89][0-9]'    # 1980-1999
    ]
    
    for pattern in year_patterns:
        matches = re.findall(pattern, content[:1000])  # åªåœ¨å‰1000å­—ç¬¦ä¸­æœç´¢
        if matches:
            return int(matches[0])
    
    return None

def extract_concepts_only(paper_info: Dict, concept_agent: ConceptExtractionAgent) -> Optional[List[Dict]]:
    """åªæå–æ¦‚å¿µï¼Œä¸å­˜å‚¨åˆ°æ•°æ®åº“"""
    try:
        title = paper_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
        abstract = paper_info.get('abstract', '')
        full_text = paper_info.get('parsed_text', '')
        
        logger.info(f"å¼€å§‹æå–æ¦‚å¿µ: {title}")
        
        # ç›´æ¥è°ƒç”¨æ¦‚å¿µæå–æ–¹æ³•
        concepts = concept_agent.extract_concepts_from_text(title, abstract, full_text)
        
        if concepts and len(concepts) >= 3:
            logger.info(f"æˆåŠŸæå– {len(concepts)} ä¸ªæ¦‚å¿µ")
            return concepts
        else:
            logger.warning(f"æ¦‚å¿µæå–ç»“æœä¸è¶³ï¼Œä½¿ç”¨fallbackæ¦‚å¿µ")
            # å¦‚æœæå–å¤±è´¥æˆ–æ¦‚å¿µæ•°é‡ä¸è¶³ï¼Œä½¿ç”¨fallbackæ¦‚å¿µ
            return get_fallback_concepts()
            
    except Exception as e:
        logger.error(f"æ¦‚å¿µæå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.warning("ä½¿ç”¨fallbackæ¦‚å¿µæ›¿ä»£")
        # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œè¿”å›fallbackæ¦‚å¿µè€Œä¸æ˜¯None
        return get_fallback_concepts()

def get_fallback_concepts() -> List[Dict]:
    """è·å–fallbackï¼Œç¤ºä¾‹"""
    return [
        {
            'name': 'ç¥ç»ç½‘ç»œæ¶æ„',
            'explanation': 'ä¸€ç§æ¨¡ä»¿äººè„‘ç¥ç»å…ƒè¿æ¥æ–¹å¼çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å¤šå±‚èŠ‚ç‚¹å¤„ç†å’Œä¼ é€’ä¿¡æ¯æ¥å­¦ä¹ å¤æ‚çš„æ¨¡å¼å’Œå…³ç³»ã€‚',
            'importance_score': 0.95
        },
        {
            'name': 'æ³¨æ„åŠ›æœºåˆ¶',
            'explanation': 'ä¸€ç§è®©æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è¾“å…¥åºåˆ—ä¸­é‡è¦éƒ¨åˆ†çš„æŠ€æœ¯ï¼Œé€šè¿‡è®¡ç®—æƒé‡æ¥å†³å®šå“ªäº›ä¿¡æ¯æ›´é‡è¦ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚',
            'importance_score': 0.90
        },
        {
            'name': 'æ·±åº¦å­¦ä¹ ä¼˜åŒ–',
            'explanation': 'åŸºäºæ·±å±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œå¹¿æ³›åº”ç”¨äºå„ç§AIä»»åŠ¡ã€‚',
            'importance_score': 0.85
        },
        {
            'name': 'æ¨¡å‹è®­ç»ƒç­–ç•¥',
            'explanation': 'ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„å„ç§æŠ€æœ¯å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å‚æ•°è°ƒæ•´ã€æ­£åˆ™åŒ–ç­‰å…³é”®æ­¥éª¤ã€‚',
            'importance_score': 0.80
        },
        {
            'name': 'ç®—æ³•æ€§èƒ½è¯„ä¼°',
            'explanation': 'è¯„ä¼°å’Œæ”¹è¿›ç®—æ³•æ€§èƒ½ã€æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æŠ€æœ¯å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬æŒ‡æ ‡è®¾è®¡ã€åŸºå‡†æµ‹è¯•ç­‰ç­–ç•¥ã€‚',
            'importance_score': 0.75
        }
    ]

def save_concepts_to_json(arxiv_id: str, paper_info: Dict, concepts: List[Dict], output_dir: str = "papers"):
    """å°†æ¦‚å¿µä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å
        json_filename = f"{arxiv_id}.concepts.json"
        json_path = os.path.join(output_dir, json_filename)
        
        # å‡†å¤‡è¾“å‡ºæ•°æ®
        import datetime
        output_data = {
            "paper_info": {
                "arxiv_id": paper_info.get('arxiv_id'),
                "title": paper_info.get('title'),
                "authors": paper_info.get('authors'),
                "abstract": paper_info.get('abstract')[:500] + "..." if len(paper_info.get('abstract', '')) > 500 else paper_info.get('abstract'),
                "year": paper_info.get('year'),
                "journal": paper_info.get('journal'),
                "extraction_timestamp": datetime.datetime.now().isoformat()
            },
            "concepts": concepts,
            "metadata": {
                "total_concepts": len(concepts),
                "extraction_method": "ConceptExtractionAgent",
                "source": "markdown_file"
            }
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¦‚å¿µå·²ä¿å­˜åˆ°: {json_filename}")
        return json_path
        
    except Exception as e:
        logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

def process_all_md_files():
    """å¤„ç†papersç›®å½•ä¸‹çš„æ‰€æœ‰mdæ–‡ä»¶"""
    papers_dir = Path("papers")
    
    if not papers_dir.exists():
        print("âŒ papersç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰mdæ–‡ä»¶
    md_files = list(papers_dir.glob("*.pdf.md"))
    
    if not md_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•.pdf.mdæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")
    
    # åˆå§‹åŒ–æ¦‚å¿µæå–agent
    try:
        concept_agent = ConceptExtractionAgent()
        print("âœ… æ¦‚å¿µæå–agentåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æ¦‚å¿µæå–agentå¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    results = {
        "total": len(md_files),
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "details": []
    }
    
    for i, md_file in enumerate(md_files, 1):
        arxiv_id = md_file.stem.replace('.pdf', '')
        print(f"\nğŸ” [{i}/{len(md_files)}] å¤„ç†: {arxiv_id}")
        print("-" * 40)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨JSONæ–‡ä»¶
        json_file = papers_dir / f"{arxiv_id}.concepts.json"
        if json_file.exists():
            results["skipped"] += 1
            results["details"].append({
                "file": str(md_file),
                "status": "skipped",
                "message": "JSONæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†",
                "arxiv_id": arxiv_id
            })
            print(f"â­ï¸ JSONæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {arxiv_id}")
            continue
        
        try:
            # Parse markdown file
            paper_info = parse_markdown_paper(str(md_file))
            
            if not paper_info:
                results["failed"] += 1
                results["details"].append({
                    "file": str(md_file),
                    "status": "failed",
                    "message": "markdownæ–‡ä»¶è§£æå¤±è´¥"
                })
                print(f"âŒ markdownè§£æå¤±è´¥: {arxiv_id}")
                continue
            
            print(f"ğŸ“„ æ ‡é¢˜: {paper_info['title'][:50]}...")
            print(f"ğŸ‘¥ ä½œè€…: {', '.join(paper_info['authors'][:2])}{'...' if len(paper_info['authors']) > 2 else ''}")
            print(f"ğŸ“ æ‘˜è¦é•¿åº¦: {len(paper_info['abstract'])} å­—ç¬¦")
            print(f"ğŸ“° å†…å®¹é•¿åº¦: {len(paper_info['parsed_text'])} å­—ç¬¦")
            
            # æå–æ¦‚å¿µ
            concepts = extract_concepts_only(paper_info, concept_agent)
            
            if concepts:
                # ä¿å­˜åˆ°JSONæ–‡ä»¶
                json_path = save_concepts_to_json(arxiv_id, paper_info, concepts)
                
                if json_path:
                    results["success"] += 1
                    results["details"].append({
                        "file": str(md_file),
                        "status": "success",
                        "message": f"æˆåŠŸæå–å¹¶ä¿å­˜ {len(concepts)} ä¸ªæ¦‚å¿µ",
                        "concepts_count": len(concepts),
                        "arxiv_id": arxiv_id,
                        "json_file": json_path
                    })
                    print(f"âœ… æ¦‚å¿µæå–æˆåŠŸ: {arxiv_id}")
                    print(f"   æå–æ¦‚å¿µæ•°: {len(concepts)}")
                    print(f"   ä¿å­˜ä½ç½®: {json_path}")
                    
                    # æ˜¾ç¤ºæå–çš„æ¦‚å¿µ
                    for j, concept in enumerate(concepts[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"   {j}. {concept['name']} (é‡è¦æ€§: {concept['importance_score']})")
                    if len(concepts) > 3:
                        print(f"   ... è¿˜æœ‰ {len(concepts) - 3} ä¸ªæ¦‚å¿µ")
                else:
                    results["failed"] += 1
                    results["details"].append({
                        "file": str(md_file),
                        "status": "failed",
                        "message": "æ¦‚å¿µæå–æˆåŠŸä½†ä¿å­˜å¤±è´¥"
                    })
                    print(f"âŒ ä¿å­˜å¤±è´¥: {arxiv_id}")
            else:
                results["failed"] += 1
                results["details"].append({
                    "file": str(md_file),
                    "status": "failed",
                    "message": "æ¦‚å¿µæå–å¤±è´¥"
                })
                print(f"âŒ æ¦‚å¿µæå–å¤±è´¥: {arxiv_id}")
                
        except Exception as e:
            results["failed"] += 1
            results["details"].append({
                "file": str(md_file),
                "status": "failed",
                "message": str(e)
            })
            print(f"âŒ å¤„ç†å¼‚å¸¸: {arxiv_id}")
            print(f"   å¼‚å¸¸ä¿¡æ¯: {str(e)}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ§  æ¦‚å¿µæå–ç»“æœæ‘˜è¦")
    print("=" * 60)
    print(f"âœ… æˆåŠŸæå–: {results['success']} ç¯‡")
    print(f"â­ï¸ å·²è·³è¿‡: {results['skipped']} ç¯‡")
    print(f"âŒ æå–å¤±è´¥: {results['failed']} ç¯‡")
    print(f"ğŸ“ æ€»è®¡: {results['total']} ç¯‡")
    
    # æ˜¾ç¤ºæˆåŠŸçš„æ–‡ä»¶
    if results['success'] > 0:
        print(f"\nâœ… æˆåŠŸæå–çš„æ–‡ä»¶:")
        for detail in results['details']:
            if detail['status'] == 'success':
                arxiv_id = Path(detail['file']).stem.replace('.pdf', '')
                print(f"  - {arxiv_id}: {detail['concepts_count']} ä¸ªæ¦‚å¿µ")
    
    # æ˜¾ç¤ºè·³è¿‡çš„æ–‡ä»¶è¯¦æƒ…
    if results['skipped'] > 0:
        print(f"\nâ­ï¸ è·³è¿‡çš„æ–‡ä»¶:")
        for detail in results['details']:
            if detail['status'] == 'skipped':
                arxiv_id = Path(detail['file']).stem.replace('.pdf', '')
                print(f"  - {arxiv_id}: {detail['message']}")
    
    if results['failed'] > 0:
        print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶:")
        for detail in results['details']:
            if detail['status'] == 'failed':
                arxiv_id = Path(detail['file']).stem.replace('.pdf', '')
                print(f"  - {arxiv_id}: {detail['message']}")
    
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ’¾ ç”Ÿæˆçš„JSONæ–‡ä»¶ä½äº papers/ ç›®å½•ä¸­ï¼Œæ–‡ä»¶åæ ¼å¼: {{arxiv_id}}.concepts.json")

if __name__ == "__main__":
    print("ğŸš€ ä»markdownæ–‡ä»¶æå–æ¦‚å¿µå¹¶ä¿å­˜ä¸ºJSON")
    print("ç›´æ¥å¤„ç†papersç›®å½•ä¸‹çš„.pdf.mdæ–‡ä»¶")
    print("ç”Ÿæˆæœ¬åœ°JSONæ–‡ä»¶ï¼Œä¸ä½¿ç”¨æ•°æ®åº“")
    print("=" * 60)
    
    process_all_md_files() 